{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "from HemaDataset import HemaDataset\n",
    "from HemaModel import HemaModel\n",
    "from DINOv2ForRadiology.dinov2.eval.segmentation.utils import UNetDecoder\n",
    "from DINOv2ForRadiology.dinov2.data.transforms import make_segmentation_train_transforms, make_segmentation_eval_transforms\n",
    "\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(model, data_loader, loss_fn=None, optimizer=None, scheduler=None, eval_metric=None):\n",
    "    \n",
    "    is_training = model.training\n",
    "    all_loss = []\n",
    "\n",
    "    for image, mask in data_loader:\n",
    "\n",
    "        image = image.cuda(non_blocking=True)\n",
    "        mask = mask.cuda(non_blocking=True)\n",
    "\n",
    "        output = model(image)\n",
    "\n",
    "        if is_training:\n",
    "            loss = loss_fn(output, mask)\n",
    "\n",
    "            # compute the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "        else:\n",
    "            for image, mask in data_loader:\n",
    "                image = image.cuda(non_blocking=True)\n",
    "                mask = mask.cuda(non_blocking=True)\n",
    "\n",
    "                output = model(image)\n",
    "\n",
    "                eval_metric(y_pred=output, y=mask)\n",
    "                \n",
    "    if is_training:\n",
    "        return ( sum(all_loss) / len(all_loss) ) # return average loss\n",
    "    return eval_metric # else, return evaluation metric\n",
    "\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, scheduler=None, epochs=100):\n",
    "    \n",
    "    best_dice = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        avrg_loss = one_epoch(model=model, data_loader=train_loader, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler)    \n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "        \n",
    "            model.eval()\n",
    "            print(f\"epoch: {epoch}, loss {avrg_loss}, \", end=\"\")\n",
    "            eval_metric = DiceMetric(include_background=False, reduction=\"none\") # bug with reduction \"mean,\" will do it manually.\n",
    "\n",
    "            eval_metric = one_epoch(model=model, data_loader=val_loader, eval_metric=eval_metric)\n",
    "\n",
    "            dice = eval_metric.aggregate().mean(axis=1).mean(axis=0).item() # take average across classes first (channels), then across batch. \n",
    "            print(f\"dice: {dice}\") \n",
    "\n",
    "            if dice > best_dice:\n",
    "                best_dice = dice\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "    return best_model, best_dice\n",
    "\n",
    "def tune_lr(lrs, model, train_loader, val_loader, loss_fn, optimizer, scheduler=None, epochs=100):\n",
    "    results = {}\n",
    "    for lr in lrs:\n",
    "        _, dice = train(model=model, train_loader=train_loader, val_loader=val_loader, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                        scheduler=scheduler, epochs=epochs)\n",
    "        results[lr] = dice\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "root = f\"{os.getcwd() + os.sep}data{os.sep}Dataset011_Cell\"\n",
    "\n",
    "train_image_transform, train_target_transform  = make_segmentation_train_transforms(resize_size=image_size)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=image_size)\n",
    "\n",
    "train_dataset = HemaDataset(root=root, split=\"train\", seg_entire_cell=True, image_transform=train_image_transform, target_transform=train_target_transform)\n",
    "val_dataset = HemaDataset(root=root, split=\"val\", seg_entire_cell=True, image_transform=eval_image_transform, target_transform=eval_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "epoch_length = math.ceil(len(train_dataset) / batch_size)\n",
    "max_iter = epoch_length * epochs \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').cuda()\n",
    "decoder = UNetDecoder(in_channels=encoder.embed_dim, out_channels=3, image_size=224, resize_image=True).cuda()\n",
    "model = HemaModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)    \n",
    "\n",
    "optim_param_groups = [\n",
    "    {\"params\": encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": decoder.parameters(), \"lr\": 1e-2}\n",
    "]\n",
    "\n",
    "loss_fn = DiceCELoss(sigmoid=True)\n",
    "optimizer = torch.optim.SGD(optim_param_groups, momentum=0.9, weight_decay=0)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6, 5e-7, 1e-7]\n",
    "results = {}\n",
    "\n",
    "train(model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
