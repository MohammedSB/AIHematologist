{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baharoon/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch\n",
    "from HemaDataset import HemaDataset\n",
    "from HemaModel import HemaModel\n",
    "from DINOv2ForRadiology.dinov2.eval.segmentation.utils import UNetDecoder\n",
    "from DINOv2ForRadiology.dinov2.data.transforms import make_segmentation_train_transforms, make_segmentation_eval_transforms\n",
    "from DINOv2ForRadiology.dinov2.models.transunet import VisionTransformer as ViT_seg\n",
    "from DINOv2ForRadiology.dinov2.models.transunet import CONFIGS as CONFIGS_ViT_seg\n",
    "\n",
    "from monai.losses.dice import DiceLoss, DiceCELoss\n",
    "from monai.metrics import DiceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor):\n",
    "    \"\"\"\n",
    "    Displays a PyTorch tensor as an image.\n",
    "\n",
    "    Parameters:\n",
    "    - tensor: a PyTorch tensor of shape (C, H, W) for an image.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if tensor needs to be normalized\n",
    "    if tensor.dtype == torch.float32:\n",
    "        if tensor.max() <= 1.0:\n",
    "            tensor = tensor\n",
    "        else:\n",
    "            # Assuming the max value represents white\n",
    "            tensor = tensor / 255.0\n",
    "    # Convert tensor to numpy\n",
    "    np_image = tensor.numpy()\n",
    "    \n",
    "    # Convert from C x H x W to H x W x C for matplotlib\n",
    "    if np_image.shape[0] == 1: # grayscale\n",
    "        np_image = np_image.squeeze()  # remove channel dim\n",
    "        plt.imshow(np_image, cmap='gray')\n",
    "    else:\n",
    "        np_image = np.transpose(np_image, (1, 2, 0))\n",
    "        plt.imshow(np_image)\n",
    "    \n",
    "    plt.axis('off')  # No axes for a cleaner image\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def one_epoch(model, data_loader, loss_fn=None, optimizer=None, scheduler=None, eval_metric=None):\n",
    "    \n",
    "    is_training = model.training\n",
    "    all_loss = []\n",
    "\n",
    "    for image, mask in data_loader:\n",
    "\n",
    "        image = image.cuda(non_blocking=True)\n",
    "        mask = mask.cuda(non_blocking=True)\n",
    "\n",
    "        output = model(image)\n",
    "\n",
    "        if is_training:\n",
    "            loss = loss_fn(output, mask)\n",
    "\n",
    "            # compute the gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "        else:\n",
    "            eval_metric(y_pred=output, y=mask)\n",
    "                \n",
    "    if is_training:\n",
    "        return ( sum(all_loss) / len(all_loss) ) # return average loss\n",
    "    return eval_metric # else, return evaluation metric\n",
    "\n",
    "def train(model, train_loader, val_loader, loss_fn, optimizer, scheduler=None, epochs=100):\n",
    "    \n",
    "    best_dice = 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        avrg_loss = one_epoch(model=model, data_loader=train_loader, loss_fn=loss_fn, optimizer=optimizer, scheduler=scheduler)    \n",
    "        \n",
    "        if epoch % 10 == 0 or epoch == (epochs-1):\n",
    "        \n",
    "            model.eval()\n",
    "            print(f\"epoch: {epoch}, loss {avrg_loss}, \", end=\"\")\n",
    "            eval_metric = DiceMetric(include_background=False, reduction=\"none\") # bug with reduction \"mean,\" will do it manually.\n",
    "\n",
    "            eval_metric = one_epoch(model=model, data_loader=val_loader, eval_metric=eval_metric)\n",
    "\n",
    "            dice = eval_metric.aggregate().mean(axis=1).mean(axis=0).item() # take average across classes first (channels), then across batch. \n",
    "            print(f\"dice: {dice}\") \n",
    "\n",
    "            if dice > best_dice:\n",
    "                best_dice = dice\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "    return best_model, best_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 448\n",
    "root = f\"{os.getcwd() + os.sep}data{os.sep}Dataset011_Cell\"\n",
    "\n",
    "train_image_transform, train_target_transform  = make_segmentation_train_transforms(resize_size=image_size)\n",
    "eval_image_transform, eval_target_transform  = make_segmentation_eval_transforms(resize_size=image_size)\n",
    "\n",
    "train_dataset = HemaDataset(root=root, split=\"train\", seg_entire_cell=True, image_transform=train_image_transform, target_transform=train_target_transform)\n",
    "val_dataset = HemaDataset(root=root, split=\"val\", seg_entire_cell=True, image_transform=eval_image_transform, target_transform=eval_target_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 100\n",
    "epoch_length = math.ceil(len(train_dataset) / batch_size)\n",
    "max_iter = epoch_length * epochs \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "loss_fn = DiceLoss(sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_vit = CONFIGS_ViT_seg[\"R50-ViT-B_16\"]\n",
    "config_vit.n_skip = 3\n",
    "config_vit.is_pretrain = True\n",
    "config_vit.n_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lrs = [5e-4, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6, 5e-7, 1e-7]\n",
    "results = {}\n",
    "\n",
    "for lr in lrs:\n",
    "    \n",
    "    encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').cuda()\n",
    "    decoder = UNetDecoder(in_channels=encoder.embed_dim, out_channels=3, image_size=448, resize_image=True).cuda()\n",
    "    model = HemaModel(encoder=encoder, decoder=decoder)\n",
    " \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "        model = torch.nn.DataParallel(model)    \n",
    "        \n",
    "    optim_param_groups = [\n",
    "        {\"params\": encoder.parameters(), \"lr\": lr},\n",
    "        {\"params\": decoder.parameters(), \"lr\": 1e-2}\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(optim_param_groups, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter, eta_min=0)\n",
    "\n",
    "    _, dice = train(model=model, train_loader=train_loader, val_loader=val_loader, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                scheduler=scheduler, epochs=epochs)\n",
    "    \n",
    "    results[lr] = dice\n",
    "    print(f\"best result for lr {lr} is {dice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('dinov2_val.json', 'w') as f:\n",
    "#     # Use json.dump to write the dictionary to the file\n",
    "#     json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/baharoon/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using layers [ 1  3  6  8 11] to get UNet skip connections.\n",
      "Using 2 GPUs!\n",
      "epoch: 0, loss 0.5438459937771162, dice: 0.48086756467819214\n",
      "epoch: 10, loss 0.47253525132934254, dice: 0.28014451265335083\n",
      "epoch: 20, loss 0.45688839505116147, dice: 0.8930609226226807\n",
      "epoch: 30, loss 0.44655093799034756, dice: 0.9019090533256531\n",
      "epoch: 40, loss 0.4493237075706323, dice: 0.908482551574707\n",
      "epoch: 50, loss 0.4559318944811821, dice: 0.9130682945251465\n",
      "epoch: 60, loss 0.450972984234492, dice: 0.9212315678596497\n",
      "epoch: 70, loss 0.4469568294783433, dice: 0.9262842535972595\n",
      "epoch: 80, loss 0.4475233865280946, dice: 0.9305381774902344\n",
      "epoch: 90, loss 0.45198361327250797, dice: 0.9312506914138794\n",
      "epoch: 100, loss 0.4476187663773696, dice: 0.9316806793212891\n",
      "epoch: 110, loss 0.45104623461763066, dice: 0.9309989213943481\n",
      "epoch: 120, loss 0.44836990783611935, dice: 0.9315680265426636\n",
      "epoch: 130, loss 0.4476891818145911, dice: 0.9003208875656128\n",
      "epoch: 140, loss 0.45029227435588837, dice: 0.9403809309005737\n",
      "epoch: 150, loss 0.4473134030898412, dice: 0.9331327080726624\n",
      "epoch: 160, loss 0.44802433997392654, dice: 0.9246656894683838\n",
      "epoch: 170, loss 0.4468431497613589, dice: 0.9196140766143799\n",
      "epoch: 180, loss 0.4468942967553933, dice: 0.9147389531135559\n",
      "epoch: 190, loss 0.4473131944735845, dice: 0.9336203932762146\n",
      "epoch: 200, loss 0.4542582457264264, dice: 0.8439082503318787\n",
      "epoch: 210, loss 0.4553600611786048, dice: 0.9121900200843811\n",
      "epoch: 220, loss 0.45058970153331757, dice: 0.9366558194160461\n",
      "epoch: 230, loss 0.4488045622905095, dice: 0.9362211227416992\n",
      "epoch: 240, loss 0.4497887020309766, dice: 0.9322441220283508\n",
      "epoch: 250, loss 0.4485209832588832, dice: 0.9362565279006958\n",
      "epoch: 260, loss 0.4474284661312898, dice: 0.93534916639328\n",
      "epoch: 270, loss 0.4481927963594596, dice: 0.9382639527320862\n",
      "epoch: 280, loss 0.44979822759826976, dice: 0.9400618076324463\n",
      "epoch: 290, loss 0.4472152665257454, dice: 0.939525842666626\n",
      "epoch: 300, loss 0.4523879873255889, dice: 0.939030647277832\n",
      "epoch: 310, loss 0.44840993111332256, dice: 0.9393157958984375\n",
      "epoch: 320, loss 0.4473633940021197, dice: 0.936911404132843\n",
      "epoch: 330, loss 0.44553011159102124, dice: 0.9405756592750549\n",
      "epoch: 340, loss 0.44837285826603573, dice: 0.9416571855545044\n",
      "epoch: 350, loss 0.44834375257293385, dice: 0.9338865280151367\n",
      "epoch: 360, loss 0.44281719252467155, dice: 0.9432928562164307\n",
      "epoch: 370, loss 0.4480240357418855, dice: 0.8320613503456116\n",
      "epoch: 380, loss 0.4492450753847758, dice: 0.9238214492797852\n",
      "epoch: 390, loss 0.4503149092197418, dice: 0.9344663619995117\n",
      "epoch: 400, loss 0.4469922110438347, dice: 0.9318378567695618\n",
      "epoch: 410, loss 0.4477792667845885, dice: 0.936066210269928\n",
      "epoch: 420, loss 0.4457908148566882, dice: 0.9383125305175781\n",
      "epoch: 430, loss 0.45022498319546383, dice: 0.9401490688323975\n",
      "epoch: 440, loss 0.44330965851744014, dice: 0.9416731595993042\n",
      "epoch: 450, loss 0.44339849924047786, dice: 0.94003826379776\n",
      "epoch: 460, loss 0.4494982548058033, dice: 0.9361950755119324\n",
      "epoch: 470, loss 0.4458051125208537, dice: 0.9424978494644165\n",
      "epoch: 480, loss 0.4525716428955396, dice: 0.9427909851074219\n",
      "epoch: 490, loss 0.44577164947986603, dice: 0.9411171674728394\n",
      "epoch: 500, loss 0.44944217428565025, dice: 0.9416789412498474\n",
      "epoch: 510, loss 0.44576815888285637, dice: 0.9417719841003418\n",
      "epoch: 520, loss 0.44443292543292046, dice: 0.9414656162261963\n",
      "epoch: 530, loss 0.4478001371026039, dice: 0.9424063563346863\n",
      "epoch: 540, loss 0.4475352317094803, dice: 0.94502192735672\n",
      "epoch: 550, loss 0.44864630699157715, dice: 0.9407598376274109\n",
      "epoch: 560, loss 0.45142315949002904, dice: 0.9366267919540405\n",
      "epoch: 570, loss 0.44305507466197014, dice: 0.9306042194366455\n",
      "epoch: 580, loss 0.446680162101984, dice: 0.9367951154708862\n",
      "epoch: 590, loss 0.4455497165520986, dice: 0.9447968602180481\n",
      "epoch: 600, loss 0.44996776183446247, dice: 0.8865675330162048\n",
      "epoch: 610, loss 0.44816428050398827, dice: 0.941489040851593\n",
      "epoch: 620, loss 0.4451192493240039, dice: 0.9382113814353943\n",
      "epoch: 630, loss 0.4540677492817243, dice: 0.940669059753418\n",
      "epoch: 640, loss 0.4480477124452591, dice: 0.9443326592445374\n",
      "epoch: 650, loss 0.44719957436124486, dice: 0.9409236907958984\n",
      "epoch: 660, loss 0.45013250783085823, dice: 0.943115770816803\n",
      "epoch: 670, loss 0.448238130658865, dice: 0.944088339805603\n",
      "epoch: 680, loss 0.4450628285606702, dice: 0.9446417689323425\n",
      "epoch: 690, loss 0.445177610963583, dice: 0.9445266723632812\n",
      "epoch: 700, loss 0.453938049574693, dice: 0.9443920254707336\n",
      "epoch: 710, loss 0.4512212984263897, dice: 0.944682776927948\n",
      "epoch: 720, loss 0.4423546530306339, dice: 0.9438790082931519\n",
      "epoch: 730, loss 0.4488759860396385, dice: 0.9442034959793091\n",
      "epoch: 740, loss 0.4477628866831462, dice: 0.9406536817550659\n",
      "epoch: 750, loss 0.4458538517355919, dice: 0.9467242956161499\n",
      "epoch: 760, loss 0.4475839013854663, dice: 0.9482516050338745\n",
      "epoch: 770, loss 0.45021942133704823, dice: 0.941082775592804\n",
      "epoch: 780, loss 0.4519755666454633, dice: 0.9366997480392456\n",
      "epoch: 790, loss 0.44328930725653964, dice: 0.9347422122955322\n",
      "epoch: 800, loss 0.44728287185231846, dice: 0.9376785755157471\n",
      "epoch: 810, loss 0.44322821994622547, dice: 0.9438301920890808\n",
      "epoch: 820, loss 0.45324665183822316, dice: 0.9459279179573059\n",
      "epoch: 830, loss 0.45381129905581474, dice: 0.9469815492630005\n",
      "epoch: 840, loss 0.4513503449658553, dice: 0.9434385299682617\n",
      "epoch: 850, loss 0.45112630104025203, dice: 0.9453297853469849\n",
      "epoch: 860, loss 0.44887372727195424, dice: 0.9467846751213074\n",
      "epoch: 870, loss 0.44796329488356906, dice: 0.9465195536613464\n",
      "epoch: 880, loss 0.4503691842158635, dice: 0.9474695920944214\n",
      "epoch: 890, loss 0.44656772290666896, dice: 0.9477730989456177\n",
      "epoch: 900, loss 0.4415750900904338, dice: 0.9476805925369263\n",
      "epoch: 910, loss 0.4493611479798953, dice: 0.9481590986251831\n",
      "epoch: 920, loss 0.44520259524385136, dice: 0.9481039047241211\n",
      "epoch: 930, loss 0.44567133113741875, dice: 0.9496656656265259\n",
      "epoch: 940, loss 0.44610566024978954, dice: 0.9469716548919678\n",
      "epoch: 950, loss 0.4426572745045026, dice: 0.9463744759559631\n",
      "epoch: 960, loss 0.45109225809574127, dice: 0.9472526907920837\n",
      "epoch: 970, loss 0.4535101503133774, dice: 0.944621741771698\n",
      "epoch: 980, loss 0.4474265972773234, dice: 0.9439417123794556\n",
      "epoch: 990, loss 0.4450409648319085, dice: 0.9413933157920837\n",
      "epoch: 999, loss 0.44476818293333054, dice: 0.9402554035186768\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "encoder = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14').cuda()\n",
    "decoder = UNetDecoder(in_channels=encoder.embed_dim, out_channels=3, image_size=448, resize_image=True).cuda()\n",
    "model = HemaModel(encoder=encoder, decoder=decoder)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)    \n",
    "\n",
    "optim_param_groups = [\n",
    "    {\"params\": encoder.parameters(), \"lr\": 1e-5},\n",
    "    {\"params\": decoder.parameters(), \"lr\": 1e-2}\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(optim_param_groups, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iter, eta_min=0)\n",
    "\n",
    "best_model, dice = train(model=model, train_loader=train_loader, val_loader=val_loader, loss_fn=loss_fn, optimizer=optimizer,\n",
    "            scheduler=scheduler, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, \"dinov2-unet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
